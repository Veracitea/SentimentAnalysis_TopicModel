{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2df5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import dns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5772149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '5.0.13', 'gitVersion': 'cfb7690563a3144d3d1175b3a20c2ec81b662a8f', 'modules': ['enterprise'], 'allocator': 'tcmalloc', 'javascriptEngine': 'mozjs', 'sysInfo': 'deprecated', 'versionArray': [5, 0, 13, 0], 'bits': 64, 'debug': False, 'maxBsonObjectSize': 16777216, 'storageEngines': ['devnull', 'ephemeralForTest', 'inMemory', 'queryable_wt', 'wiredTiger'], 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1667726219, 5), 'signature': {'hash': b'\\xff\\xfb\\x83[m\\xe5\\xac\\xdf\\xa8\\x0e\\xa8\\x04\\x8f}\\\\\\x81\\x84\\x19\\xaeT', 'keyId': 7123661958628245507}}, 'operationTime': Timestamp(1667726219, 5)}\n"
     ]
    }
   ],
   "source": [
    "conn_str = \"mongodb+srv://tartiniglia:W.I.T.C.H.@atlascluster.tv8xjir.mongodb.net/?retryWrites=true&w=majority\"\n",
    "# set a 5-second connection timeout\n",
    "client = pymongo.MongoClient(conn_str, serverSelectionTimeoutMS=5000)\n",
    "print(client.server_info()) # just a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7730b957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '5.0.13', 'gitVersion': 'cfb7690563a3144d3d1175b3a20c2ec81b662a8f', 'modules': ['enterprise'], 'allocator': 'tcmalloc', 'javascriptEngine': 'mozjs', 'sysInfo': 'deprecated', 'versionArray': [5, 0, 13, 0], 'bits': 64, 'debug': False, 'maxBsonObjectSize': 16777216, 'storageEngines': ['devnull', 'ephemeralForTest', 'inMemory', 'queryable_wt', 'wiredTiger'], 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1666493689, 33), 'signature': {'hash': b'\\x0f\\x90b\\xf6\\xb4\\xb0x\\xb3\\x82a\\xe3\\xbf\\n\\xa63\\x97ZC\\xcf\\r', 'keyId': 7123661958628245507}}, 'operationTime': Timestamp(1666493689, 33)}\n"
     ]
    }
   ],
   "source": [
    "conn_str = \"mongodb+srv://datapdtzpeeps:mercuryVENUSace@atlascluster.tv8xjir.mongodb.net/?retryWrites=true&w=majority\"\n",
    "# set a 5-second connection timeout\n",
    "client = pymongo.MongoClient(conn_str, serverSelectionTimeoutMS=5000)\n",
    "print(client.server_info()) # just a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd220c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bookEater', 'admin', 'local']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58d0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = client[\"bookEater\"] ##MongoDB waits until you have created a collection (table), with at least one document (record) before it actually creates the database (and collection).\n",
    "mycol = mydb[\"Books2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdc7c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 16:28:51.074992: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/python/v3.8.5/lib:/apps/openmpi-4.0.5/lib:/apps/aocl/aocc/2.2/lib:/apps/aocc-compiler-2.2.0/lib32:/apps/aocc-compiler-2.2.0/lib:/opt/rocm/lib\n",
      "2022-11-06 16:28:51.075028: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/D190003/CZ1016/base/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3172: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "2022-11-06 16:28:55.679554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/python/v3.8.5/lib:/apps/openmpi-4.0.5/lib:/apps/aocl/aocc/2.2/lib:/apps/aocc-compiler-2.2.0/lib32:/apps/aocc-compiler-2.2.0/lib:/opt/rocm/lib\n",
      "2022-11-06 16:28:55.679586: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-06 16:28:55.679604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dsaicn01): /proc/driver/nvidia/version does not exist\n",
      "2022-11-06 16:28:55.679827: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import scrapy\n",
    "from scrapy import Selector\n",
    "from scrapy.crawler import CrawlerProcess, CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "import  collections\n",
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from collections import OrderedDict\n",
    "df = pd.read_csv('Books.csv')\n",
    "# function to add to JSON\n",
    "def write_json(new_data, filename='test.json'):\n",
    "    with open(filename,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"book_details\"].append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    " \n",
    "    # python object to be appended\n",
    "# y = {\"emp_name\":\"Nikhil\",\n",
    "#      \"email\": \"nikhil@geeksforgeeks.org\",\n",
    "#      \"job_profile\": \"Full Time\"\n",
    "#     }\n",
    "     \n",
    "#write_json(y)\n",
    "\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "model = load_model('best_model.h5')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "def predict_class(text):\n",
    "    '''Function to predict sentiment class of the passed text'''\n",
    "    \n",
    "    sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "    max_len=63\n",
    "    \n",
    "    # Transforms text to a sequence of integers using a tokenizer object\n",
    "    xt = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
    "    # Do the prediction using the loaded model\n",
    "    yt = model.predict(xt).argmax(axis=1)\n",
    "    # Print the predicted sentiment\n",
    "    return(sentiment_classes[yt[0]])\n",
    "\n",
    "failed=[]\n",
    "mmn=df['ISBN'].tolist()[:1000:]\n",
    "\n",
    "class Spidey( scrapy.Spider ):\n",
    "    name = \"spidisRgdPETS\"\n",
    "    def start_requests( self ):\n",
    "        for index,row in df.iterrows():\n",
    "            print(index)\n",
    "            request= scrapy.Request( url = \"https://www.goodreads.com/book/isbn/\"+str(row['ISBN']),meta = {\n",
    "                  'dont_redirect': False,\n",
    "                  'handle_httpstatus_list': [302]}, callback = self.parse)\n",
    "            request.cb_kwargs['ocean']=str(row['ISBN'])\n",
    "            request.cb_kwargs['ocean1']=str(row['Book-Title'])\n",
    "            request.cb_kwargs['ocean2']=str(row['Book-Author'])\n",
    "            request.cb_kwargs['ocean3']=str(row['Year-Of-Publication'])\n",
    "            request.cb_kwargs['ocean4']=str(row['Image-URL-L'])\n",
    "            request.cb_kwargs['ocean5']=str(row['Publisher'])\n",
    "            yield request\n",
    "    def parse( self, response, ocean, ocean1, ocean2, ocean3, ocean4, ocean5):\n",
    "        init={}\n",
    "        init[\"ISBN\"]=ocean\n",
    "        init[\"Book-Title\"]=ocean1\n",
    "        init[\"Book-Author\"]=ocean2\n",
    "        init[\"Year-Of-Publication\"]=ocean3\n",
    "        init[\"Image-URL-L\"]=ocean4\n",
    "        init[\"Publisher\"]=ocean5\n",
    "        init[\"URL\"] =response.url\n",
    "        print(response.url)\n",
    "        k=response.xpath('/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[3]/div/span[2]/text()').getall()\n",
    "        if k==[]:\n",
    "            k=response.xpath('/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[3]/div/span[2]/p/text()').getall() \n",
    "            if k==[]:\n",
    "                k=response.xpath('/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[3]/div/span/text()').getall()\n",
    "        j=response.xpath('//*[@id=\"bookReviews\"]/div[1]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "        p=response.xpath('//*[@id=\"bookReviews\"]/div[3]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "        q=response.xpath('//*[@id=\"bookReviews\"]/div[5]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "        r=response.xpath('//*[@id=\"bookReviews\"]/div[7]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "        d=response.xpath('//*[@id=\"bookReviews\"]/div[9]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "        try:\n",
    "            d1=response.xpath('//*[@id=\"bookReviews\"]/div[11]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d2=response.xpath('//*[@id=\"bookReviews\"]/div[13]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d3=response.xpath('//*[@id=\"bookReviews\"]/div[15]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d4=response.xpath('//*[@id=\"bookReviews\"]/div[17]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d5=response.xpath('//*[@id=\"bookReviews\"]/div[19]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d6=response.xpath('//*[@id=\"bookReviews\"]/div[21]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d7=response.xpath('//*[@id=\"bookReviews\"]/div[23]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d8=response.xpath('//*[@id=\"bookReviews\"]/div[25]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d9=response.xpath('//*[@id=\"bookReviews\"]/div[27]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "            d10=response.xpath('//*[@id=\"bookReviews\"]/div[29]//div/div/div[1]/div[2]/span/span[2]/text()').getall()\n",
    "        except:\n",
    "            d1=\"\"\n",
    "            d2=\"\"\n",
    "            d3=\"\"\n",
    "            d4=\"\"\n",
    "            d5=\"\"\n",
    "            d6=\"\"\n",
    "            d7=\"\"\n",
    "            d8=\"\"\n",
    "            d9=\"\"\n",
    "            d10=\"\"\n",
    "        pss=[''.join(lis) for lis in [j,p,q,r,d,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10] if lis!=\"\"]\n",
    "        init[\"Review\"] = pss\n",
    "        print(\"work\")\n",
    "        try:\n",
    "            print(\"work1s\")\n",
    "            topic_model = BERTopic()\n",
    "            topics, probs = topic_model.fit_transform(pss)\n",
    "            init[\"Review Topics\"]=topic_model.generate_topic_labels()\n",
    "            print(\"work1\")\n",
    "\n",
    "        except: \n",
    "            init[\"Review Topics\"] = np.nan\n",
    "            print(\"work1.5\")\n",
    "\n",
    "        init[\"Review Sentiment\"] = [predict_class([x]) for x in pss]\n",
    "        print(\"work2\")\n",
    "\n",
    "        init[\"Genre\"] = list(np.unique(np.array(response.xpath('//*[@class=\"actionLinkLite bookPageGenreLink\"]/text()').getall())))\n",
    "        if k==[]:\n",
    "            failed.append(ocean)\n",
    "#             #failed=list(np.unique(np.array(failed)))\n",
    "#             mmn.append(ocean)\n",
    "            print(ocean)\n",
    "        else:\n",
    "            init[\"Summary\"]=k\n",
    "            write_json(init)\n",
    "            print(\"yes\")\n",
    "            x = mycol.insert_one(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfc2f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.48972.dsaihn01/ipykernel_741286/3946183173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtop2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/top2vec/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtop2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTop2Vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.0.27'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/hdbscan/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhdbscan_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHDBSCAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrobust_single_linkage_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustSingleLinkage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobust_single_linkage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidity_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from .prediction import (approximate_predict,\n\u001b[1;32m      5\u001b[0m                          \u001b[0mmembership_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from ._hdbscan_linkage import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msingle_linkage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmst_linkage_core\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mhdbscan/_hdbscan_linkage.pyx\u001b[0m in \u001b[0;36minit hdbscan._hdbscan_linkage\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296cf1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: top2vec[sentence_encoders] in /home/D190003/CZ1016/base/lib/python3.8/site-packages (1.0.27)\n",
      "Requirement already satisfied: pandas in /apps/python/v3.8.5/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (1.2.0)\n",
      "Requirement already satisfied: wordcloud in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (1.8.2.2)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in /home/D190003/.local/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (0.8.29)\n",
      "Requirement already satisfied: umap-learn>=0.5.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (0.5.3)\n",
      "Requirement already satisfied: gensim>=4.0.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (1.20.3)\n",
      "Requirement already satisfied: tensorflow in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_encoders]) (2.7.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (5.2.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.0.0)\n",
      "Requirement already satisfied: cython>=0.27 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.26)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec[sentence_encoders]) (2.1.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.8)\n",
      "Requirement already satisfied: tqdm in /apps/python/v3.8.5/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (4.56.0)\n",
      "Requirement already satisfied: numba>=0.49 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.54.1)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (58.5.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /apps/python/v3.8.5/lib/python3.8/site-packages (from pandas->top2vec[sentence_encoders]) (2020.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from pandas->top2vec[sentence_encoders]) (2.7.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->top2vec[sentence_encoders]) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (4.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/D190003/.local/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.3.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.4.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (12.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.36.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.44.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.21.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorflow->top2vec[sentence_encoders]) (2.7.0)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (1.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (1.7.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /apps/python/v3.8.5/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (3.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /apps/python/v3.8.5/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (4.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /apps/python/v3.8.5/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (2022.6.15)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard~=2.6->tensorflow->top2vec[sentence_encoders]) (2.1.1)\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 33.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 75.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "\u001b[K     |█████████████████████████████▊  | 536.2 MB 17.1 MB/s eta 0:00:03     |█████████                       | 163.2 MB 17.3 MB/s eta 0:00:24"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 23.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 511.8 MB 53 kB/s s eta 0:00:01     |████████████████████████▌       | 392.6 MB 16.9 MB/s eta 0:00:08\n",
      "\u001b[?25h  Using cached tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "  Downloading tensorflow-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 498.4 MB 46 kB/s eta 0:00:32\n",
      "\u001b[?25h  Downloading tensorflow-2.8.2-cp38-cp38-manylinux2010_x86_64.whl (498.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 498.0 MB 47 kB/s s eta 0:00:01\n",
      "\u001b[?25h  Downloading tensorflow-2.8.1-cp38-cp38-manylinux2010_x86_64.whl (498.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 498.0 MB 51 kB/s s eta 0:00:01\n",
      "\u001b[?25h  Downloading tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 497.6 MB 4.3 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.8.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 28.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tensorflow_text-2.7.3-cp38-cp38-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 136.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from wordcloud->top2vec[sentence_encoders]) (3.6.1)\n",
      "Requirement already satisfied: pillow in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from wordcloud->top2vec[sentence_encoders]) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (21.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /apps/python/v3.8.5/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.0.5)\n",
      "Installing collected packages: flatbuffers, tensorflow-hub, tensorflow-text\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 22.10.26\n",
      "    Not uninstalling flatbuffers at /apps/python/v3.8.5/lib/python3.8/site-packages, outside environment /home/D190003/CZ1016/base\n",
      "    Can't uninstall 'flatbuffers'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-rocm 2.10.0.520 requires keras<2.11,>=2.10.0, but you have keras 2.7.0 which is incompatible.\n",
      "tensorflow-rocm 2.10.0.520 requires libclang>=13.0.0, but you have libclang 12.0.0 which is incompatible.\n",
      "tensorflow-rocm 2.10.0.520 requires tensorboard<2.11,>=2.10, but you have tensorboard 2.7.0 which is incompatible.\n",
      "tensorflow-rocm 2.10.0.520 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.7.0 which is incompatible.\n",
      "tensorflow-rocm 2.10.0.520 requires tensorflow-io-gcs-filesystem>=0.23.1, but you have tensorflow-io-gcs-filesystem 0.21.0 which is incompatible.\u001b[0m\n",
      "Successfully installed flatbuffers-2.0.7 tensorflow-hub-0.12.0 tensorflow-text-2.7.3\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/D190003/CZ1016/base/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: top2vec[sentence_transformers] in /home/D190003/CZ1016/base/lib/python3.8/site-packages (1.0.27)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (1.20.3)\n",
      "Requirement already satisfied: umap-learn>=0.5.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (0.5.3)\n",
      "Requirement already satisfied: pandas in /apps/python/v3.8.5/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (1.2.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in /home/D190003/.local/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (0.8.29)\n",
      "Requirement already satisfied: gensim>=4.0.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (4.2.0)\n",
      "Requirement already satisfied: wordcloud in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (1.8.2.2)\n",
      "Requirement already satisfied: sentence-transformers in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (2.2.2)\n",
      "Requirement already satisfied: torch in /apps/python/v3.8.5/lib/python3.8/site-packages (from top2vec[sentence_transformers]) (1.13.0+rocm5.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from gensim>=4.0.0->top2vec[sentence_transformers]) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from gensim>=4.0.0->top2vec[sentence_transformers]) (5.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.0.0)\n",
      "Requirement already satisfied: cython>=0.27 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (0.29.26)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec[sentence_transformers]) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /apps/python/v3.8.5/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (4.56.0)\n",
      "Requirement already satisfied: numba>=0.49 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.54.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.5.8)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_transformers]) (58.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from pandas->top2vec[sentence_transformers]) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /apps/python/v3.8.5/lib/python3.8/site-packages (from pandas->top2vec[sentence_transformers]) (2020.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->top2vec[sentence_transformers]) (1.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from sentence-transformers->top2vec[sentence_transformers]) (4.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from sentence-transformers->top2vec[sentence_transformers]) (0.10.1)\n",
      "Requirement already satisfied: torchvision in /apps/python/v3.8.5/lib/python3.8/site-packages (from sentence-transformers->top2vec[sentence_transformers]) (0.14.0+rocm5.2)\n",
      "Requirement already satisfied: nltk in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from sentence-transformers->top2vec[sentence_transformers]) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from sentence-transformers->top2vec[sentence_transformers]) (0.1.96)\n",
      "Requirement already satisfied: requests in /apps/python/v3.8.5/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (4.4.0)\n",
      "Requirement already satisfied: filelock in /apps/python/v3.8.5/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (3.0.12)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2.4.7)\n",
      "Requirement already satisfied: sacremoses in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->top2vec[sentence_transformers]) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->top2vec[sentence_transformers]) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->top2vec[sentence_transformers]) (0.10.3)\n",
      "Requirement already satisfied: click in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from nltk->sentence-transformers->top2vec[sentence_transformers]) (8.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (1.26.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from torchvision->sentence-transformers->top2vec[sentence_transformers]) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from wordcloud->top2vec[sentence_transformers]) (3.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /apps/python/v3.8.5/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (4.37.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (1.0.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/D190003/CZ1016/base/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: top2vec[indexing] in /home/D190003/CZ1016/base/lib/python3.8/site-packages (1.0.27)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in /home/D190003/.local/lib/python3.8/site-packages (from top2vec[indexing]) (0.8.29)\n",
      "Requirement already satisfied: gensim>=4.0.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[indexing]) (4.2.0)\n",
      "Requirement already satisfied: pandas in /apps/python/v3.8.5/lib/python3.8/site-packages (from top2vec[indexing]) (1.2.0)\n",
      "Requirement already satisfied: umap-learn>=0.5.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[indexing]) (0.5.3)\n",
      "Requirement already satisfied: wordcloud in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[indexing]) (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from top2vec[indexing]) (1.20.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from gensim>=4.0.0->top2vec[indexing]) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from gensim>=4.0.0->top2vec[indexing]) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[indexing]) (1.1.1)\n",
      "Requirement already satisfied: cython>=0.27 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[indexing]) (0.29.26)\n",
      "Requirement already satisfied: joblib>=1.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from hdbscan>=0.8.27->top2vec[indexing]) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec[indexing]) (2.1.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[indexing]) (0.5.8)\n",
      "Requirement already satisfied: numba>=0.49 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[indexing]) (0.54.1)\n",
      "Requirement already satisfied: tqdm in /apps/python/v3.8.5/lib/python3.8/site-packages (from umap-learn>=0.5.1->top2vec[indexing]) (4.56.0)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[indexing]) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[indexing]) (58.5.3)\n",
      "Collecting hnswlib\n",
      "  Downloading hnswlib-0.6.2.tar.gz (31 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /apps/python/v3.8.5/lib/python3.8/site-packages (from pandas->top2vec[indexing]) (2020.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from pandas->top2vec[indexing]) (2.7.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->top2vec[indexing]) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from wordcloud->top2vec[indexing]) (3.6.1)\n",
      "Requirement already satisfied: pillow in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from wordcloud->top2vec[indexing]) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[indexing]) (21.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[indexing]) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[indexing]) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /apps/python/v3.8.5/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[indexing]) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[indexing]) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from matplotlib->wordcloud->top2vec[indexing]) (1.3.1)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.6.2-cp38-cp38-linux_x86_64.whl size=1193658 sha256=3ebc5bfa2aa9758bc9fb8c881480631cc7cef25f1f391cda000698c0de7dbcb0\n",
      "  Stored in directory: /home/D190003/.cache/pip/wheels/74/3b/89/4bd924865709f24ac2df457bcca4c0b55a3eb89c5a94ce3ce8\n",
      "Successfully built hnswlib\n",
      "Installing collected packages: hnswlib\n",
      "Successfully installed hnswlib-0.6.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/D190003/CZ1016/base/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install top2vec[sentence_encoders]\n",
    "!pip install top2vec[sentence_transformers]\n",
    "!pip install top2vec[indexing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595d2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 16:40:29.851230: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/python/v3.8.5/lib:/apps/openmpi-4.0.5/lib:/apps/aocl/aocc/2.2/lib:/apps/aocc-compiler-2.2.0/lib32:/apps/aocc-compiler-2.2.0/lib:/opt/rocm/lib\n",
      "2022-11-24 16:40:29.851266: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c67722b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a11cb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26179a2846444aa9aef24c0d5ae36ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069ce02136e34f96a4c3daeeba7571e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d751dd40e5044ad945622049237e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bc26bffc974e43babaebb2e6d08f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38da82c31c04e8dadf6b44aba5fc20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad4b0130b9840f083f102aea37a08bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd0906351f6424ea055bf90fddd1715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44cc1e5f5b64198b5d0bc41f9481222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db37aded51224b9b8195487db69ed6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d1c30c1f344084b655d828fc3ae14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974f2d1e806a4ae982f815e09a040580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39819382ac24739ba648d2f16734c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d0f9df4ef8437887313e54da4ea60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168b008ee3bd42519dbeb5b8922c42e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #270: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.48972.dsaihn01/ipykernel_759594/599430679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_topic_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guided_topic_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_dimensionality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# Cluster reduced embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m                 logger.info(\"The dimensionality reduction algorithm did not contain the `y` parameter and\"\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2684\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2685\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2715\u001b[0m         \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \"\"\"\n\u001b[0;32m-> 2717\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2718\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a42d91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e537f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(doc)\n",
    "k=topic_model.generate_topic_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3010dacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>-1_the_of_and_to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count              Name\n",
       "0     -1     26  -1_the_of_and_to"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bb5749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(doc)\n",
    "k=topic_model.generate_topic_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a895e5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>-1_the_of_and_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>15</td>\n",
       "      <td>41_jesus_divinity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count               Name\n",
       "0     -1     26   -1_the_of_and_to\n",
       "1     41     15  41_jesus_divinity"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f82e01cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 16:12:33 [sentence_transformers.SentenceTransformer] INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2022-11-06 16:12:33 [sentence_transformers.SentenceTransformer] INFO: Use pytorch device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.48972.dsaihn01/ipykernel_806123/3075438860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_topic_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_topic_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guided_topic_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_dimensionality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# Cluster reduced embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m                 logger.info(\"The dimensionality reduction algorithm did not contain the `y` parameter and\"\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2684\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2685\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2715\u001b[0m         \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \"\"\"\n\u001b[0;32m-> 2717\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2718\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(doc)\n",
    "k=topic_model.generate_topic_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e496741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1_the_of_to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e0f9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>-1_the_of_to_and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count              Name\n",
       "0     -1     23  -1_the_of_to_and"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f111240b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:32:47,097 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-11-06 15:32:47,101 - top2vec - INFO - Creating joint document/word embedding\n",
      "Exception in thread Thread-123:\n",
      "Traceback (most recent call last):\n",
      "  File \"/apps/python/v3.8.5/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/apps/python/v3.8.5/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/D190003/CZ1016/base/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1163, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"/home/D190003/CZ1016/base/lib/python3.8/site-packages/gensim/models/doc2vec.py\", line 424, in _do_train_job\n",
      "    tally += train_document_dbow(\n",
      "  File \"gensim/models/doc2vec_inner.pyx\", line 358, in gensim.models.doc2vec_inner.train_document_dbow\n",
      "TypeError: Cannot convert list to numpy.ndarray\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.48972.dsaihn01/ipykernel_759594/3522387149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtop2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, min_count, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating joint document/word embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'doc2vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdoc2vec_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_normed_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         super(Doc2Vec, self).__init__(\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    428\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_doctags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         super(Doc2Vec, self).train(\n\u001b[0m\u001b[1;32m    517\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1071\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m   1432\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python/v3.8.5/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python/v3.8.5/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec\n",
    "\n",
    "model = Top2Vec(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61eb0a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.48972.dsaihn01/ipykernel_759594/3118592352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I read this when it came out-- I thought I had written a review --no instead I wrote about a wonderful lasting reading impression\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Secrets from mother and daughters always come out from these\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal-sentence-encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, min_count, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_import_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pre-processing documents for training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CZ1016/base/lib/python3.8/site-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36m_check_import_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muse_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAVE_TENSORFLOW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                 raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n\u001b[0m\u001b[1;32m   1090\u001b[0m                                   \u001b[0;34m\"Try: pip install top2vec[sentence_encoders]\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                                   \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
      "\u001b[0;31mImportError\u001b[0m: universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text"
     ]
    }
   ],
   "source": [
    "model = Top2Vec([\"I read this when it came out-- I thought I had written a review --no instead I wrote about a wonderful lasting reading impression\",\"Secrets from mother and daughters always come out from these\"], embedding_model='universal-sentence-encoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e100d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e866d84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: hdbscan 0.8.29\n",
      "Uninstalling hdbscan-0.8.29:\n",
      "  Successfully uninstalled hdbscan-0.8.29\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall hdbscan --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5391813c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /apps/python/v3.8.5/lib/python3.8/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Installing collected packages: gensim\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cogdl 0.5.2 requires gensim<4.0, but you have gensim 4.2.0 which is incompatible.\u001b[0m\n",
      "Successfully installed gensim-4.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/D190003/CZ1016/base/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6ab92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.2 MB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from hdbscan) (1.4.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from hdbscan) (1.0.0)\n",
      "Requirement already satisfied: cython>=0.27 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan) (0.29.26)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/D190003/CZ1016/base/lib/python3.8/site-packages (from hdbscan) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /apps/python/v3.8.5/lib/python3.8/site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp38-cp38-linux_x86_64.whl size=2171982 sha256=7f4ec8e623d5b16189347057906b557ae07cb8a5364dc9ccfcbbbc1fa6be041d\n",
      "  Stored in directory: /var/tmp/pbs.48972.dsaihn01/pip-ephem-wheel-cache-kfx_wn32/wheels/76/06/48/527e038689c581cc9e519c73840efdc7473805149e55bd7ffd\n",
      "Successfully built hdbscan\n",
      "Installing collected packages: hdbscan\n",
      "Successfully installed hdbscan-0.8.29\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/D190003/CZ1016/base/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan --no-cache-dir --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05a29cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "model = load_model('best_model.h5')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "def predict_class(text):\n",
    "    '''Function to predict sentiment class of the passed text'''\n",
    "    \n",
    "    sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "    max_len=63\n",
    "    \n",
    "    # Transforms text to a sequence of integers using a tokenizer object\n",
    "    xt = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
    "    # Do the prediction using the loaded model\n",
    "    yt = model.predict(xt).argmax(axis=1)\n",
    "    # Print the predicted sentiment\n",
    "    print('The predicted sentiment is', sentiment_classes[yt[0]])\n",
    "    return(sentiment_classes[yt[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81230a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sentiment is negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=predict_class(['I hate exams'])\n",
    "type(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
